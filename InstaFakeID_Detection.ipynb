{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vidhan-tiwari/Fake_instagram_account_detection/blob/main/InstaFakeID_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "id": "8qhi90Wg4fXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHr8RphtQIjI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYnxGGQLPAzA"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"nahiar/instagram_bot_detection\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHQKMhd1QAIR"
      },
      "outputs": [],
      "source": [
        "df = dataset['train'].to_pandas()\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "Nr3KvCpw5uN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shuffle the DF before applying train test split\n",
        "df_shuffled = df.sample(frac = 1, random_state = 42).reset_index(drop = True)\n",
        "\n",
        "splitting_point = int(0.8*len(df))\n",
        "train_df = df_shuffled.iloc[:splitting_point]\n",
        "test_df = df_shuffled.iloc[splitting_point:]"
      ],
      "metadata": {
        "id": "6hikm0KC52ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J03R8lQiVR66"
      },
      "outputs": [],
      "source": [
        "train_df = train_df.sample(frac = 1,random_state = 42).reset_index(drop = True)\n",
        "test_df = test_df.sample(frac = 1,random_state = 42).reset_index(drop = True )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0OkO94CSLCE"
      },
      "outputs": [],
      "source": [
        "train_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPb_Lae9SS9o"
      },
      "outputs": [],
      "source": [
        "test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWj_qGMTYeH4"
      },
      "outputs": [],
      "source": [
        "len(train_df),len(test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzvfMk3KU-LX"
      },
      "source": [
        "Here \"nums/length\" and nums/length_full_name\" is ratio of numerical characters in its user name and its full name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wayp72-XvelL"
      },
      "outputs": [],
      "source": [
        "# plot the distribution of all numeric features\n",
        "binary_columns = [\"profile pic\",\"name==username\",\"external URL\",\"private\",\"fake\"]\n",
        "non_binary_columns = [col for col in train_df.columns if col not in binary_columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4-Hs1B8W4S2"
      },
      "outputs": [],
      "source": [
        "len(non_binary_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMCWkivUMfdG"
      },
      "outputs": [],
      "source": [
        "non_binary_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fu10ZUP9YWa4"
      },
      "outputs": [],
      "source": [
        "# calculating skewness of all the attributes\n",
        "train_df[non_binary_columns].skew()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ifItdemBIOK"
      },
      "outputs": [],
      "source": [
        "selected_columns = ['profile pic','nums/length username','fullname words','nums/length fullname','description length','external URL','#posts','#follows']\n",
        "columns_to_drop = ['name==username','private','#followers']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selected_non_binary_columns = [col for col in selected_columns if col in non_binary_columns]\n",
        "selected_non_binary_columns"
      ],
      "metadata": {
        "id": "72jhjCcG5oWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in selected_non_binary_columns:\n",
        "  print(f\"{col} : skewness = {train_df[col].skew()} , range = {train_df[col].min()} - {train_df[col].max()}\")"
      ],
      "metadata": {
        "id": "ZQKm7M5x6Mys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applying transformation techniques first"
      ],
      "metadata": {
        "id": "U1ZvH0hA5ee5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PowerTransformer\n",
        "def transform(df, pt_train_data = None):\n",
        "  # Explicitly create a copy to avoid SettingWithCopyWarnin\n",
        "  df = df.copy()\n",
        "  pt = PowerTransformer(method='yeo-johnson') # Initialize pt here\n",
        "\n",
        "  if pt_train_data is None:\n",
        "    df[non_binary_columns] = pt.fit_transform(df[non_binary_columns])\n",
        "  else:\n",
        "    pt = pt_train_data # Use provided transformer\n",
        "    df[non_binary_columns] = pt.transform(df[non_binary_columns])\n",
        "\n",
        "\n",
        "  return df,pt # Return the modified DataFrame"
      ],
      "metadata": {
        "id": "fnLjcKPKqbB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_train_df ,pt_train_data= transform(train_df)"
      ],
      "metadata": {
        "id": "pvax2W3wr-2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_train_df[non_binary_columns].skew()"
      ],
      "metadata": {
        "id": "meCLM8y6r17I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8Ths8Vh82Q_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_test_df,pt_train_data  = transform(test_df,pt_train_data)"
      ],
      "metadata": {
        "id": "CnSVA8npvi1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4R-KhzemNL-f"
      },
      "outputs": [],
      "source": [
        "new_test_df[non_binary_columns].skew()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA SCALING"
      ],
      "metadata": {
        "id": "80Ihw7U8e-yC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "def scale_dataframe(df, scaler = None):\n",
        "    df_scaled = df.copy()\n",
        "    columns = df_scaled.columns\n",
        "\n",
        "\n",
        "    if scaler is None:\n",
        "      scaler = StandardScaler()\n",
        "      df_scaled_values = scaler.fit_transform(df_scaled)\n",
        "    else:\n",
        "      df_scaled_values = scaler.transform(df_scaled)\n",
        "    df_scaled = pd.DataFrame(df_scaled_values, columns=columns)\n",
        "\n",
        "    return df_scaled, scaler"
      ],
      "metadata": {
        "id": "1GWaEdStfEps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = new_train_df.drop('fake',axis = 1)\n",
        "y_train = new_train_df['fake']\n",
        "X_test = new_test_df.drop('fake',axis = 1)\n",
        "y_test = new_test_df['fake']"
      ],
      "metadata": {
        "id": "0Y1RElGJ-6hC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train ,scaler_train_data = scale_dataframe(X_train)"
      ],
      "metadata": {
        "id": "7JbStyAwgC-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test ,scaler_train_data = scale_dataframe(X_test,scaler_train_data)"
      ],
      "metadata": {
        "id": "bsaP4OSz_lO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5udBPufTSzp"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWSIXcyzKFqf"
      },
      "source": [
        "# Applying Logistic regression on symmmetric data #\n",
        "- Using Yeo-Johnson power transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wm_8KMn8KD5W"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kwu5oxeUU7Wm"
      },
      "source": [
        "# Logistic regression without sklearn (Custom CODE) #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLdOS6L33LBR"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return 1.0 / (1.0 + np.exp(-x))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost_vectorized(X, w, b, y):\n",
        "  m = X.shape[0]\n",
        "  z = X @ w + b  # (m,n) @ (n,) -> (m,).\n",
        "  f_wb = sigmoid(z) # (m,)\n",
        "\n",
        "  cost = -y * np.log(f_wb) - (1 - y) * np.log(1 - f_wb)\n",
        "  total_cost = np.sum(cost) / m\n",
        "\n",
        "  return total_cost\n"
      ],
      "metadata": {
        "id": "muyv_PeEibPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_functions_vectorized(X, w, b, y):\n",
        "  m, n = X.shape\n",
        "  z = X @ w + b      # (m,) vector of z for all examples\n",
        "  f_wb = sigmoid(z)  # (m,) vector of all predictions\n",
        "\n",
        "  error = f_wb - y   # (m,) vector of all errors\n",
        "\n",
        "  # (m,) * (m,n) is not what we want ,we  need (n,)\n",
        "  # s0 we do (n,m) @ (m,) -> (n,)\n",
        "  dj_dw = (X.T @ error) / m # transpose of X(m,n) is X.T(n,m)\n",
        "  dj_db = np.sum(error) / m\n",
        "\n",
        "  return dj_dw, dj_db"
      ],
      "metadata": {
        "id": "PEMvlrEdjO2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_vectorized(X, y, alpha, num_iters):\n",
        "  cost_history =[]\n",
        "  w = np.zeros(X.shape[1])\n",
        "  b = 0\n",
        "  for i in range(num_iters):\n",
        "    dj_dw, dj_db = gradient_functions_vectorized(X, w, b, y)\n",
        "    w = w - alpha * dj_dw\n",
        "    b = b - alpha * dj_db\n",
        "\n",
        "    if i % 5000 == 0:\n",
        "        cost = compute_cost_vectorized(X, w, b, y)\n",
        "        cost_history.append(cost)\n",
        "        print(f\"Iteration {i:5d}: Cost {cost:0.4f}\")\n",
        "\n",
        "  return w, b, cost_history\n"
      ],
      "metadata": {
        "id": "rHI7GIp2jU_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_vectorized(X, w, b):\n",
        "  z = X @ w + b\n",
        "  f_wb = sigmoid(z)\n",
        "  p = f_wb >= 0.5\n",
        "  return p.astype(int)"
      ],
      "metadata": {
        "id": "gOSGeNxUjtkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w, b,cost_history = train_vectorized(X_train.values,y_train,0.01,50000)"
      ],
      "metadata": {
        "id": "F3zB5a_Pj7-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = predict_vectorized(X_test.values,w,b)"
      ],
      "metadata": {
        "id": "-OopTCyckB9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7N4aBP4nJ4G"
      },
      "outputs": [],
      "source": [
        "accuracy_score(y_test,y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNSo79RjC-4C"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUCtTO3X0cKk"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(\n",
        "    range(len(cost_history)),\n",
        "    cost_history,\n",
        "    color='blue',\n",
        "    linestyle='solid',\n",
        "    linewidth=2\n",
        ")\n",
        "\n",
        "plt.xlabel('Iteration', fontsize=12)\n",
        "plt.ylabel('Cost', fontsize=12)\n",
        "plt.title('Learning Curve (Cost vs. Iterations)', fontsize=14)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THmQ5-vMLjWo"
      },
      "outputs": [],
      "source": [
        "w,b"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now_training Logistic Model of Logistic Regression (ridge (l2 regularized) and lasso(l1 regularized)"
      ],
      "metadata": {
        "id": "ZTwClSTPc1oK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lasso_regression = LogisticRegression(penalty = 'l1',solver = 'liblinear',C = 1.0,max_iter = 100000,random_state = 42)\n",
        "lasso_regression.fit(X_train,y_train)\n",
        "y_pred = lasso_regression.predict(X_test)\n",
        "accuracy_score(y_test,y_pred)\n"
      ],
      "metadata": {
        "id": "BkhELk1wfMf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W = lasso_regression.coef_\n",
        "b = lasso_regression.intercept_\n",
        "W,b"
      ],
      "metadata": {
        "id": "yBK11on2fufc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W_lasso = np.array([-2.08145489,  1.84867303, -0.03856457,  0.29341673,  0.40523131,\n",
        "         -0.34613844, -0.49467169, -0.53865868, -1.21093493, -4.67090307,\n",
        "          1.85291686])\n",
        "b_lasso = 0.72040019"
      ],
      "metadata": {
        "id": "2swYkhKaBhT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "for col in train_df.columns:\n",
        "  print(f\"{col}  : {i}\")\n",
        "  i += 1"
      ],
      "metadata": {
        "id": "5w20ekzlEAak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c38e69a"
      },
      "source": [
        "from sklearn.preprocessing import PowerTransformer\n",
        "def process_and_predict_selected(record_selected,pt, scaler,w, b,threshold):\n",
        "    \"\"\"\n",
        "    record_selected : 1-D numpy array containing ONLY the `selected_columns`\n",
        "                      and in that exact order.\n",
        "    \"\"\"\n",
        "\n",
        "    record_selected = record_selected.astype(float)\n",
        "    # convert np array into dataframe\n",
        "    # reshape 1D array into a 2D array with one row\n",
        "    df_selected = pd.DataFrame(record_selected.reshape(1, -1), columns = X_train.columns)\n",
        "    # transform the data_frame using up-written transform function\n",
        "    transformed_selected_df,pt  = transform(df_selected,pt)\n",
        "    # scale the data_frame using up_written scale function\n",
        "    scaled_transformed_df ,scaler = scale_dataframe(transformed_selected_df,scaler)\n",
        "\n",
        "\n",
        "    prob_fake = sigmoid(scaled_transformed_df.values @ w + b)[0]\n",
        "    is_not_fake = prob_fake < threshold\n",
        "\n",
        "    return is_not_fake, prob_fake"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.columns"
      ],
      "metadata": {
        "id": "NhADut2_phyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_records = {\n",
        "    \"Person 1\": np.array([1, 0.25,  7, 0, 0, 19, 0, 1, 0, 277, 220]), # person_1\n",
        "    \"Person 2\": np.array([1, 0.17, 22, 0, 0, 37, 0, 1, 7, 343, 308]), # person_2\n",
        "    \"Person 3\": np.array([1, 0.16, 13, 0, 0, 0,  0, 1, 0, 302, 736]), # person_3\n",
        "    \"Person 4\": np.array([0, 0.11, 17, 0, 0, 0,  0, 1, 0, 127, 247]), # person_4\n",
        "    \"Random 1\": np.array([0, 0.05,  6, 0.15, 0, 32, 1,0, 34567,78, 69978]) # randomly selected\n",
        "}"
      ],
      "metadata": {
        "id": "U__Ec_GaaWL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 0.50\n",
        "for name, rec in all_records.items():\n",
        "    ok, p = process_and_predict_selected(rec, pt_train_data, scaler_train_data, W_lasso, b_lasso,threshold)\n",
        "    result = \"Not-Fake \" if ok else \"Fake \"\n",
        "    print(f\"{name:10s} â†’ {result}  (prob_fake = {p:.4f})\")"
      ],
      "metadata": {
        "id": "djNDngY7aPdH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM37SrOA7ksXnEezn3J8te+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}